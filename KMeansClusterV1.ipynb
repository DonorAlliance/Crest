{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Welcome To Machine Learning Algorithms** \n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *K Means Clustering*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the necessary libraries into the file. \n",
    "- Pandas is a very useful library for working with data and includes functions for analyzing and manipulating data.\n",
    "- scikit-learn (sklearn) is the library that contains the tools and functions for machine learning and analysis.\n",
    "- numpy is a general purpose library used for creating and working with arrays\n",
    "- matplotlib is used to create graphs and visualizations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to read the data we want to analyze. we can manually enter data into a script but the Pandas library has a function to read a csv file which is an easy way to read a large dataset into memory. we first save our file path into a variable (file_path) and use this as the perameter for the read_csv() function. If the csv file is saved to the same directory as the script we only need to use the name and extension of the file and makes reading the file simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "file_path = 'atendees.csv'  # Update this with the actual path to your Excel file\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code is preparing the data in a format that can be understood by the machine learing algorithm. The k means clustering algorithm only accepts numeric values and the data must use the same scale to avoid a bias toward values with a higer magnitude. \n",
    "- The work specialty column forms discrete groups so numeric codes are used to represent the distinct groups. (e.g. all data = 1, clinical = 2, etc.) \n",
    "- A continuous or numeric variable such as years of experience may have a much larger magnitude so we can to use the StandarScaler() function from the scikit-learn library to normalize the data. (This function basically sets the average of the dataset to 0 and transforms the values so that is has a standard deviation of 1)\n",
    "\n",
    "Prining the results to the console allows us to look for errors in our data. Small spelling or capitalization errors can lead to skewed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the work specialty\n",
    "data['work_specialty_encoded'] = data['work_specialty'].astype('category').cat.codes\n",
    "\n",
    "# Normalize the years of experience\n",
    "scaler = StandardScaler()\n",
    "data['experience_normalized'] = scaler.fit_transform(data[['years_of_experience']])\n",
    "\n",
    "# Prepare features for clustering\n",
    "features = data[['experience_normalized', 'work_specialty_encoded']]\n",
    "\n",
    "# Prints the results of the preparation\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For simplcity we are manually setting the number of clusters we want to see, but in a situation where you do not know how many clusters to use you would use a technique such as the Elbow Method or the Silhouette Score to determine how many clusters ideally represent the dataset. The following code is an example of the Elbow method which plots the variation between using different numbers of clusters. The 'elbow' on the graph is where adding more clusters does not add any informantion and risks over-fitting the model and sub dividing actual groups.*\n",
    "\n",
    "- how many natural groups do you think this data set forms? what should we use for K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to find the optimal number of clusters\n",
    "inertia = []\n",
    "K = range(1, 11)  # Test cluster numbers from 1 to 10\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section actually applies the k means algorithm to the prepared data. \n",
    "- The n_clusters parameter determine sets the number of clusters that will be calculated (we are using 3 here but you would determine this number based on the results of the elbow method or similar analysis). \n",
    "- The random state initializes the random number generator behind the scenes so it does not matter what the value is as long as it remains the same across multiple runs of the code it will allow you to validate the results.\n",
    "\n",
    "This randomly assigns the the centroids then as we add each data point we calculate the distance to the centroid and assign that point to the closest cluster. Then the centroid is recalculated to represent the center of the cluster until all data points have been assigned to a cluster. This is the final step in creating our clusters that are as similar as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "data['cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Calculate distances to the cluster centroids\n",
    "data['distance_to_centroid'] = np.linalg.norm(features.values - kmeans.cluster_centers_[data['cluster']], axis=1)\n",
    "\n",
    "# Sort individuals within each cluster by distance to the centroid\n",
    "data = data.sort_values(by=['cluster', 'distance_to_centroid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is creating three empty buckets (arrays) and then looping throught the clusters and assigning evenly distribuing each cluster into the buckets. we essentially created the most similar groups we can and then evenly split them into balanced groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the balanced groups\n",
    "groups = [[] for _ in range(3)]\n",
    "\n",
    "# Distribute individuals to ensure balanced groups\n",
    "for i, person in data.iterrows():\n",
    "    smallest_group = min(groups, key=len)\n",
    "    smallest_group.append(person)\n",
    "\n",
    "# Assign group numbers to the data\n",
    "data['balanced_group'] = None\n",
    "for group_number, group in enumerate(groups):\n",
    "    for person in group:\n",
    "        data.loc[person.name, 'balanced_group'] = group_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses the matplotlib library to plot the results of the groups on a two dimentional graph. More than two variables can be used with this algorithm but it makes visualization difficult. There are many ways to customize and label the graph but these are some of the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters and centroids\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Colors for the clusters\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(3):\n",
    "    cluster_data = data[data['cluster'] == i]\n",
    "    plt.scatter(cluster_data['experience_normalized'], cluster_data['work_specialty_encoded'], c=colors[i], label=f'Cluster {i}', alpha=0.5)\n",
    "\n",
    "# Plot centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='yellow', marker='X', label='Centroids')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Normalized Years of Experience')\n",
    "plt.ylabel('Work Specialty Encoded')\n",
    "plt.title('K-Means Clustering of Individuals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we print the results to the console. The balanced_group column represnts the buckets we created to hold the evenly distributed clusters and labels each row with a group number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting balanced groups\n",
    "print(data[['name', 'years_of_experience', 'work_specialty', 'balanced_group']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just used the k means clusters machine learning algrithm to group data with multiple variables into similar clusters and distribute them into balanced groups!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading:\n",
    "- https://en.wikipedia.org/wiki/K-means_clustering\n",
    "- https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "- https://www.geeksforgeeks.org/k-means-clustering-introduction/\n",
    "- http://varianceexplained.org/r/kmeans-free-lunch/\n",
    "- https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n",
    "- https://medium.com/@evgen.ryzhkov/5-stages-of-data-preprocessing-for-k-means-clustering-b755426f9932\n",
    "- https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7#:~:text=We%20can%20clearly%20see%20that,like%20KNN%20or%20K%2DMeans.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- https://www.baeldung.com/cs/k-means-flaws-improvements\n",
    "- https://datarundown.com/k-means-clustering-pros-cons/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
